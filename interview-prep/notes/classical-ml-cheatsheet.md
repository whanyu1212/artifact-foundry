# Classical Machine Learning Cheatsheet

> Comprehensive reference for classical ML algorithms, theory, and best practices
> Based on Stanford CS-229 course materials

## Table of Contents
1. [Fundamentals](#fundamentals)
2. [Supervised Learning](#supervised-learning)
3. [Unsupervised Learning](#unsupervised-learning)
4. [Model Evaluation & Selection](#model-evaluation--selection)
5. [Learning Theory](#learning-theory)
6. [Regularization & Optimization](#regularization--optimization)
7. [Bias-Variance Tradeoff](#bias-variance-tradeoff)
8. [Algorithm Selection Guide](#algorithm-selection-guide)
   - [Scikit-Learn Flowchart](#scikit-learn-algorithm-selection-flowchart)

---

## Fundamentals

### Core Concepts

**Supervised Learning**: Build predictors using labeled training data `{x^(1), ..., x^(m)}` paired with `{y^(1), ..., y^(m)}`
- **Regression**: Continuous outcomes
- **Classification**: Discrete class predictions

**Unsupervised Learning**: Find hidden patterns in unlabeled data `{x^(1), ..., x^(m)}`

**Model Types**:
- **Discriminative Models**: Directly estimate `P(y|x)` - learn decision boundaries
- **Generative Models**: Estimate `P(x|y)` and use Bayes' rule - learn probability distributions

### Key Notation

| Symbol | Meaning |
|--------|---------|
| `m` | Number of training examples |
| `n` | Number of features |
| `x^(i)` | i-th training input |
| `y^(i)` | i-th training output |
| `h_Î¸(x)` | Hypothesis function |
| `Î¸` | Model parameters |
| `Î±` | Learning rate |

### Loss Functions

| Loss Function | Formula | Use Case |
|---------------|---------|----------|
| **Least Squared Error** | `1/2(y-z)Â²` | Linear regression |
| **Logistic Loss** | `log(1+exp(-yz))` | Logistic regression |
| **Hinge Loss** | `max(0, 1-yz)` | SVM |
| **Cross-Entropy** | `-[yÂ·log(z) + (1-y)Â·log(1-z)]` | Neural networks |

**Cost Function**: `J(Î¸) = Î£_{i=1}^m L(h_Î¸(x^(i)), y^(i))`

### Optimization Algorithms

**Gradient Descent**: `Î¸ â† Î¸ - Î±Â·âˆ‡J(Î¸)`

**Newton's Method**: `Î¸ â† Î¸ - â„“'(Î¸)/â„“''(Î¸)`
- Faster convergence but requires computing Hessian

**Likelihood Maximization**: `Î¸^opt = arg max_Î¸ L(Î¸)`

### Important Inequalities

**Jensen's Inequality**: For convex function `f` and random variable `X`:
```
E[f(X)] â‰¥ f(E[X])
```

**Union Bound**: `P(Aâ‚ âˆª ... âˆª Aâ‚–) â‰¤ P(Aâ‚) + ... + P(Aâ‚–)`

**Hoeffding Inequality**: For `m` i.i.d. Bernoulli variables with parameter `Ï†`:
```
P(|Ï† - Ï†Ì‚| > Î³) â‰¤ 2Â·exp(-2Î³Â²m)
```

---

## Supervised Learning

### Linear Regression

**Assumption**: `y|x;Î¸ ~ N(Î¼, ÏƒÂ²)`

**Hypothesis**: `h_Î¸(x) = Î¸^T x`

**Normal Equations** (closed-form solution):
```
Î¸ = (X^T X)^(-1) X^T y
```

**LMS Algorithm** (Widrow-Hoff rule):
```
âˆ€j: Î¸_j â† Î¸_j + Î±Â·Î£[y^(i) - h_Î¸(x^(i))]Â·x_j^(i)
```

**Locally Weighted Regression (LWR)**:
- Weight training examples: `w^(i)(x) = exp(-|x^(i) - x|Â²/(2Ï„Â²))`
- `Ï„`: bandwidth parameter controlling locality

**Pros/Cons**:
- âœ… Simple, interpretable, closed-form solution
- âœ… Works well with linear relationships
- âŒ Assumes linearity
- âŒ Sensitive to outliers

---

### Logistic Regression

**Sigmoid Function**:
```
g(z) = 1/(1 + e^(-z)) âˆˆ (0,1)
```

**Model**: `P(y=1|x;Î¸) = Ï† = g(Î¸^T x)`

**Assumption**: `y|x;Î¸ ~ Bernoulli(Ï†)`

**Decision Boundary**: `Î¸^T x = 0`

**No closed-form solution** - use gradient descent or Newton's method

**Softmax Regression** (multiclass):
```
Ï†_i = exp(Î¸_i^T x) / Î£_{j=1}^K exp(Î¸_j^T x)
```

**Pros/Cons**:
- âœ… Probabilistic output
- âœ… Works well for binary/multiclass
- âœ… Less prone to outliers than linear regression
- âŒ Assumes linear decision boundary
- âŒ Can underfit complex relationships

---

### Generalized Linear Models (GLM)

**Exponential Family Distribution**:
```
p(y;Î·) = b(y)Â·exp(Î·Â·T(y) - a(Î·))
```

Where:
- `Î·`: natural parameter
- `T(y)`: sufficient statistic
- `a(Î·)`: log partition function
- `b(y)`: base measure

**Common Distributions**:

| Distribution | Î· | T(y) | a(Î·) |
|--------------|---|------|------|
| **Bernoulli** | `log(Ï†/(1-Ï†))` | `y` | `log(1+exp(Î·))` |
| **Gaussian** | `Î¼` | `y` | `Î·Â²/2` |
| **Poisson** | `log(Î»)` | `y` | `e^Î·` |

**GLM Assumptions**:
1. `y|x;Î¸ ~ ExpFamily(Î·)`
2. `h_Î¸(x) = E[y|x;Î¸]`
3. `Î· = Î¸^T x` (linear in parameters)

**Applications**:
- Bernoulli â†’ Logistic regression
- Gaussian â†’ Linear regression
- Poisson â†’ Count data modeling

---

### Support Vector Machines (SVM)

**Goal**: Maximize margin (minimum distance to decision boundary)

**Decision Function**: `h(x) = sign(w^T x - b)`

**Optimization Problem**:
```
min 1/2Â·||w||Â²
subject to: y^(i)(w^T x^(i) - b) â‰¥ 1, âˆ€i
```

**Decision Boundary**: `w^T x - b = 0`

**Margin**: `2/||w||`

**Hinge Loss**: `L(z,y) = max(0, 1-yz) = [1-yz]_+`

**Soft-Margin SVM** (with slack variables):
```
min 1/2Â·||w||Â² + CÂ·Î£Î¾_i
subject to: y^(i)(w^T x^(i) - b) â‰¥ 1 - Î¾_i, Î¾_i â‰¥ 0
```

**Kernels**: `K(x,z) = Ï†(x)^T Ï†(z)`

Common kernels:
- **Linear**: `K(x,z) = x^T z`
- **Polynomial**: `K(x,z) = (x^T z + c)^d`
- **Gaussian (RBF)**: `K(x,z) = exp(-||x-z||Â²/(2ÏƒÂ²))`
- **Sigmoid**: `K(x,z) = tanh(ÎºÂ·x^T z + c)`

**Lagrangian Formulation**:
```
L(w,b,Î±) = 1/2Â·||w||Â² - Î£Î±_i[y^(i)(w^T x^(i) - b) - 1]
```

**Dual Problem**:
```
max Î£Î±_i - 1/2Â·Î£Î£Î±_i Î±_j y^(i) y^(j) K(x^(i), x^(j))
subject to: 0 â‰¤ Î±_i â‰¤ C, Î£Î±_i y^(i) = 0
```

**Pros/Cons**:
- âœ… Effective in high dimensions
- âœ… Works well with clear margin of separation
- âœ… Kernel trick enables non-linear boundaries
- âœ… Memory efficient (uses subset of training points)
- âŒ Computationally expensive for large datasets
- âŒ Sensitive to noise and overlapping classes
- âŒ No probabilistic output

---

### Gaussian Discriminant Analysis (GDA)

**Generative approach**: Model `P(x|y)` then derive `P(y|x)`

**Assumptions**:
- `y ~ Bernoulli(Ï†)`
- `x|y=0 ~ N(Î¼â‚€, Î£)`
- `x|y=1 ~ N(Î¼â‚, Î£)`

**Maximum Likelihood Estimates**:
```
Ï†Ì‚ = (1/m)Â·Î£ 1_{y^(i)=1}

Î¼Ì‚_j = Î£ 1_{y^(i)=j}Â·x^(i) / Î£ 1_{y^(i)=j}

Î£Ì‚ = (1/m)Â·Î£(x^(i) - Î¼_{y^(i)})(x^(i) - Î¼_{y^(i)})^T
```

**Bayes Rule Application**:
```
P(y=1|x) = P(x|y=1)Â·P(y=1) / [P(x|y=1)Â·P(y=1) + P(x|y=0)Â·P(y=0)]
```

**GDA vs Logistic Regression**:
- GDA makes stronger assumptions (Gaussian)
- GDA more data efficient when assumptions hold
- Logistic regression more robust to violations
- Both produce linear decision boundaries

**Pros/Cons**:
- âœ… Data efficient when assumptions hold
- âœ… Probabilistic output
- âœ… Works well with small datasets
- âŒ Strong distributional assumptions
- âŒ Assumes shared covariance
- âŒ Sensitive to outliers

---

### Naive Bayes

**Core Assumption**: Features are conditionally independent given class
```
P(x|y) = P(xâ‚|y)Â·P(xâ‚‚|y)Â·Â·Â·P(x_n|y) = Î _{i=1}^n P(x_i|y)
```

**Prediction**:
```
Å· = arg max_y P(y)Â·Î _{i=1}^n P(x_i|y)
```

**Maximum Likelihood Estimates**:
```
P(y=k) = #{j: y^(j)=k} / m

P(x_i=l|y=k) = #{j: y^(j)=k and x_i^(j)=l} / #{j: y^(j)=k}
```

**Variants**:
- **Gaussian Naive Bayes**: Continuous features, assume normal distribution
- **Multinomial Naive Bayes**: Discrete counts (text classification)
- **Bernoulli Naive Bayes**: Binary features

**Laplace Smoothing** (avoid zero probabilities):
```
P(x_i=l|y=k) = [#{matches} + 1] / [#{y=k} + |V|]
```

**Applications**:
- Text classification
- Spam detection
- Sentiment analysis
- Document categorization

**Pros/Cons**:
- âœ… Fast training and prediction
- âœ… Works well with high dimensions
- âœ… Requires small training data
- âœ… Handles missing data well
- âŒ Strong independence assumption (rarely true)
- âŒ "Naive" can hurt performance

---

### Decision Trees (CART)

**Classification and Regression Trees**

**Split Criteria**:

For **Classification**:
- **Gini Impurity**: `Gini = 1 - Î£p_iÂ²`
- **Entropy**: `H = -Î£p_iÂ·log(p_i)`
- **Information Gain**: `IG = H(parent) - Î£(|child|/|parent|)Â·H(child)`

For **Regression**:
- **MSE**: Mean squared error reduction
- **MAE**: Mean absolute error reduction

**Splitting Algorithm**:
1. Evaluate all possible feature/threshold splits
2. Choose split maximizing information gain (or minimizing impurity)
3. Recursively split child nodes
4. Stop when: max depth, min samples, pure nodes

**Pruning**:
- **Pre-pruning**: Stop growing early (max depth, min samples)
- **Post-pruning**: Grow full tree, then remove nodes (cost-complexity)

**Pros/Cons**:
- âœ… Highly interpretable
- âœ… Handles non-linear relationships
- âœ… No feature scaling needed
- âœ… Handles mixed data types
- âœ… Feature importance built-in
- âŒ Prone to overfitting
- âŒ Unstable (high variance)
- âŒ Biased toward dominant classes

---

### Ensemble Methods

#### Random Forest

**Algorithm**:
1. Bootstrap sample from training data (bagging)
2. At each split, consider random subset of features
3. Build multiple deep decision trees
4. Aggregate predictions (voting/averaging)

**Hyperparameters**:
- Number of trees (`n_estimators`)
- Max features per split (`max_features`)
- Tree depth (`max_depth`)
- Min samples per split/leaf

**Out-of-Bag (OOB) Error**:
- Use unsampled data (~37%) for validation
- No need for separate validation set

**Pros/Cons**:
- âœ… Reduces overfitting vs single tree
- âœ… Handles high-dimensional data
- âœ… Feature importance
- âœ… Robust to outliers
- âœ… Parallelizable
- âŒ Less interpretable than single tree
- âŒ Memory intensive
- âŒ Slower prediction than single tree

---

#### Boosting

**Core Idea**: Combine weak learners sequentially, focusing on mistakes

**AdaBoost (Adaptive Boosting)**:

1. Initialize weights: `w^(1)_i = 1/m`
2. For each weak learner `t`:
   - Train on weighted data
   - Compute error: `Îµ_t = Î£w_iÂ·1_{h_t(x^(i))â‰ y^(i)}`
   - Compute weight: `Î±_t = 1/2Â·log((1-Îµ_t)/Îµ_t)`
   - Update weights: `w^(t+1)_i = w^(t)_iÂ·exp(-Î±_tÂ·y^(i)Â·h_t(x^(i)))`
   - Normalize weights
3. Final prediction: `H(x) = sign(Î£Î±_tÂ·h_t(x))`

**Gradient Boosting**:

1. Initialize: `Fâ‚€(x) = arg min_Î³ Î£ L(y^(i), Î³)`
2. For each iteration `m`:
   - Compute pseudo-residuals: `r_i = -âˆ‚L(y^(i), F(x^(i)))/âˆ‚F(x^(i))`
   - Fit weak learner `h_m` to residuals
   - Find optimal step size: `Î³_m = arg min_Î³ Î£ L(y^(i), F_{m-1}(x^(i)) + Î³Â·h_m(x^(i)))`
   - Update: `F_m(x) = F_{m-1}(x) + Î³_mÂ·h_m(x)`
3. Return: `F_M(x)`

**XGBoost** (Extreme Gradient Boosting):
- Regularized objective function
- Tree pruning using depth-first approach
- Built-in handling of missing values
- Parallel processing
- Cache-aware computation

**Key Hyperparameters**:
- Learning rate (`Î·` or `learning_rate`)
- Number of estimators (`n_estimators`)
- Max depth (`max_depth`)
- Subsample ratio
- Regularization terms (L1/L2)

**Pros/Cons**:
- âœ… State-of-the-art performance
- âœ… Handles complex relationships
- âœ… Feature importance
- âœ… Less prone to overfitting than single trees
- âŒ Sensitive to hyperparameters
- âŒ Sequential (harder to parallelize)
- âŒ Prone to overfitting with noisy data
- âŒ Less interpretable

---

### k-Nearest Neighbors (k-NN)

**Non-parametric method**: No training phase, lazy learning

**Algorithm**:
1. Store all training examples
2. For new point `x`:
   - Find `k` nearest neighbors using distance metric
   - Classification: Majority vote
   - Regression: Average of neighbors' values

**Distance Metrics**:
- **Euclidean**: `d(x,z) = âˆš(Î£(x_i-z_i)Â²)`
- **Manhattan**: `d(x,z) = Î£|x_i-z_i|`
- **Minkowski**: `d(x,z) = (Î£|x_i-z_i|^p)^(1/p)`
- **Cosine**: `d(x,z) = 1 - (xÂ·z)/(||x||Â·||z||)`

**Choosing k**:
- Small `k`: Low bias, high variance (overfitting)
- Large `k`: High bias, low variance (underfitting)
- Typical: Cross-validation to select optimal `k`
- Rule of thumb: `k â‰ˆ âˆšm`

**Weighted k-NN**:
```
Weight by inverse distance: w_i = 1/d(x, x^(i))
```

**Curse of Dimensionality**:
- Performance degrades in high dimensions
- All points become equidistant
- Need exponentially more data

**Optimization**:
- **KD-Trees**: Faster nearest neighbor search
- **Ball Trees**: Better for high dimensions
- **LSH**: Locality-sensitive hashing

**Pros/Cons**:
- âœ… Simple, intuitive
- âœ… No training time
- âœ… Naturally handles multiclass
- âœ… Non-linear decision boundaries
- âŒ Slow prediction (`O(mÂ·n)` per query)
- âŒ Memory intensive (stores all data)
- âŒ Sensitive to feature scaling
- âŒ Curse of dimensionality
- âŒ Sensitive to irrelevant features

---

## Unsupervised Learning

### K-Means Clustering

**Goal**: Partition data into `k` clusters minimizing within-cluster variance

**Algorithm**:
1. **Initialize**: Randomly select `k` centroids `Î¼â‚,...,Î¼â‚–`
2. **Repeat until convergence**:
   - **Assignment**: `c^(i) = arg min_j ||x^(i) - Î¼_j||Â²`
   - **Update**: `Î¼_j = (1/|C_j|)Â·Î£_{iâˆˆC_j} x^(i)`

**Objective (Distortion Function)**:
```
J(c,Î¼) = Î£_{i=1}^m ||x^(i) - Î¼_{c^(i)}||Â²
```

**Initialization Methods**:
- **Random**: Pick `k` random points
- **K-means++**: Probabilistic selection favoring distant points
- **Multiple runs**: Run with different initializations, pick best

**Choosing k**:
- **Elbow Method**: Plot distortion vs `k`, look for "elbow"
- **Silhouette Analysis**: Measure cluster separation
- **Gap Statistic**: Compare to random data
- **Domain Knowledge**: Use prior information

**Variants**:
- **K-Medoids**: Use actual data points as centers (more robust)
- **Mini-batch K-means**: Faster for large datasets
- **Fuzzy C-Means**: Soft cluster assignments

**Pros/Cons**:
- âœ… Simple, fast, scalable
- âœ… Guaranteed to converge
- âœ… Works well with spherical clusters
- âŒ Must specify `k` beforehand
- âŒ Sensitive to initialization
- âŒ Assumes spherical, equal-sized clusters
- âŒ Sensitive to outliers
- âŒ Struggles with non-convex shapes

---

### Expectation-Maximization (EM)

**Purpose**: Maximum likelihood estimation with latent variables

**General Framework**:

**E-step**: Compute posterior probability of latent variables
```
Q_i(z^(i)) = P(z^(i)|x^(i);Î¸)
```

**M-step**: Maximize expected complete-data log-likelihood
```
Î¸ := arg max_Î¸ Î£_i Î£_{z^(i)} Q_i(z^(i))Â·log[P(x^(i),z^(i);Î¸)/Q_i(z^(i))]
```

**Convergence**: Guaranteed to increase likelihood (or stay same)

---

#### Gaussian Mixture Models (GMM)

**Model Assumptions**:
- `z ~ Multinomial(Ï†)` (which Gaussian)
- `x|z=j ~ N(Î¼_j, Î£_j)`

**E-step**: Compute responsibilities
```
w_j^(i) = P(z^(i)=j|x^(i);Î¸)
        = [Ï†_jÂ·N(x^(i);Î¼_j,Î£_j)] / [Î£_l Ï†_lÂ·N(x^(i);Î¼_l,Î£_l)]
```

**M-step**: Update parameters
```
Ï†_j = (1/m)Â·Î£_i w_j^(i)

Î¼_j = Î£_i w_j^(i)Â·x^(i) / Î£_i w_j^(i)

Î£_j = Î£_i w_j^(i)Â·(x^(i)-Î¼_j)(x^(i)-Î¼_j)^T / Î£_i w_j^(i)
```

**Advantages over K-Means**:
- Soft assignments (probabilistic)
- Can model elliptical clusters
- Provides uncertainty estimates
- Cluster sizes can vary

**Choosing Number of Components**:
- BIC (Bayesian Information Criterion)
- AIC (Akaike Information Criterion)
- Cross-validation likelihood

**Pros/Cons**:
- âœ… Probabilistic cluster assignments
- âœ… Flexible cluster shapes
- âœ… Captures cluster uncertainty
- âŒ Slower than k-means
- âŒ Can converge to local optima
- âŒ Sensitive to initialization
- âŒ More parameters to estimate

---

### Hierarchical Clustering

**Two Approaches**:
- **Agglomerative** (bottom-up): Start with individual points, merge
- **Divisive** (top-down): Start with one cluster, split

**Agglomerative Algorithm**:
1. Start: Each point is its own cluster
2. Repeat:
   - Find closest cluster pair
   - Merge them
3. Stop: When single cluster or desired number reached

**Linkage Criteria**:

| Linkage | Distance Definition | Characteristics |
|---------|-------------------|-----------------|
| **Single** | `min d(a,b)` | Produces long chains, sensitive to outliers |
| **Complete** | `max d(a,b)` | Compact clusters, sensitive to outliers |
| **Average** | `mean d(a,b)` | Balance between single/complete |
| **Ward** | Minimizes within-cluster variance | Produces equal-sized, spherical clusters |

**Dendrogram**:
- Tree diagram showing merge hierarchy
- Cut at desired height to get clusters
- Helps visualize cluster structure

**Distance Metrics**: Same as k-NN (Euclidean, Manhattan, etc.)

**Pros/Cons**:
- âœ… No need to specify `k` beforehand
- âœ… Produces hierarchical structure
- âœ… Deterministic (no random initialization)
- âœ… Works with any distance metric
- âŒ Computationally expensive: `O(mÂ²log m)` or `O(mÂ³)`
- âŒ Cannot undo merges (greedy)
- âŒ Sensitive to noise and outliers
- âŒ Difficult to scale to large datasets

---

### DBSCAN (Density-Based Clustering)

**Core Concepts**:
- **Îµ-neighborhood**: Points within distance `Îµ`
- **Core point**: Has â‰¥ `minPts` neighbors in Îµ-neighborhood
- **Border point**: In Îµ-neighborhood of core point but not core itself
- **Noise point**: Neither core nor border

**Algorithm**:
1. For each unvisited point:
   - Mark as visited
   - If core point: Create cluster, add neighbors
   - Expand cluster by visiting neighbors
2. Points not in any cluster are noise

**Hyperparameters**:
- `Îµ` (eps): Neighborhood radius
- `minPts`: Minimum points to form dense region

**Choosing Parameters**:
- **k-distance plot**: Plot distance to k-th nearest neighbor, look for "elbow"
- `minPts â‰¥ dimensions + 1`
- Domain knowledge

**Pros/Cons**:
- âœ… Finds arbitrary-shaped clusters
- âœ… Robust to outliers (marks as noise)
- âœ… No need to specify number of clusters
- âŒ Struggles with varying density
- âŒ Sensitive to `Îµ` and `minPts`
- âŒ High-dimensional curse of dimensionality

---

### Principal Component Analysis (PCA)

**Goal**: Find orthogonal directions of maximum variance

**Mathematical Foundation**:

**Eigenvalue Problem**: `Az = Î»z`
- `A`: matrix
- `z`: eigenvector
- `Î»`: eigenvalue

**Spectral Theorem**: Symmetric matrix `A` can be diagonalized:
```
A = UÎ›U^T
```
Where `U` is orthogonal matrix of eigenvectors, `Î›` is diagonal matrix of eigenvalues

**PCA Algorithm**:

1. **Standardize data**:
   ```
   xÌƒ = (x - Î¼)/Ïƒ
   ```
   (Zero mean, unit variance per feature)

2. **Compute covariance matrix**:
   ```
   Î£ = (1/m)Â·Î£_{i=1}^m x^(i)(x^(i))^T
   ```

3. **Compute eigendecomposition**:
   ```
   Î£ = UÎ›U^T
   ```
   Sort eigenvalues: `Î»â‚ â‰¥ Î»â‚‚ â‰¥ ... â‰¥ Î»_n`

4. **Select top k eigenvectors**:
   ```
   U_k = [uâ‚, uâ‚‚, ..., u_k]
   ```

5. **Project data**:
   ```
   z^(i) = U_k^T x^(i)
   ```

**Variance Explained**:
```
Variance ratio = Î£_{i=1}^k Î»_i / Î£_{j=1}^n Î»_j
```

**Choosing k**:
- Cumulative variance threshold (e.g., 95%)
- Scree plot (elbow method)
- Cross-validation on downstream task

**Reconstruction**:
```
xÌ‚^(i) = U_k z^(i)
Reconstruction error = ||x^(i) - xÌ‚^(i)||Â²
```

**Applications**:
- Dimensionality reduction
- Data visualization (2D/3D)
- Noise reduction
- Feature extraction
- Data compression

**Pros/Cons**:
- âœ… Reduces dimensionality
- âœ… Removes correlated features
- âœ… Fast computation (SVD)
- âœ… Interpretable (variance explained)
- âŒ Linear transformation only
- âŒ Assumes high variance = important
- âŒ Sensitive to feature scaling
- âŒ Components may not be interpretable

---

### Independent Component Analysis (ICA)

**Goal**: Find statistically independent source signals from mixed observations

**Model Assumption**:
```
x = As
```
Where:
- `x`: observed signals (mixed)
- `s`: source signals (independent)
- `A`: mixing matrix

**Objective**: Find unmixing matrix `W = A^(-1)` such that:
```
s = Wx
```

**Independence Assumption**:
```
P(s) = Î _{i=1}^n P(s_i)
```

**Bell-Sejnowski Algorithm**:

**Likelihood**:
```
p(x) = Î _{i=1}^n p_s(w_i^T x)Â·|det(W)|
```

**Log-likelihood**:
```
â„“(W) = Î£_{i=1}^m [Î£_j log(g'(w_j^T x^(i))) + log|det(W)|]
```

**Update Rule** (stochastic gradient ascent):
```
W := W + Î±Â·[(1-2g(Wx^(i)))(x^(i))^T + (W^T)^(-1)]
```

Where `g(s) = 1/(1+e^(-s))` is sigmoid

**Preprocessing**:
1. **Centering**: Subtract mean
2. **Whitening**: Decorrelate and normalize variance
   ```
   x_white = Î£^(-1/2)(x - Î¼)
   ```

**Contrast Functions** (alternatives to likelihood):
- **Negentropy**: `J(s) = H(s_gauss) - H(s)`
- **Kurtosis**: Measure of non-Gaussianity
- **Mutual information**: Minimize between components

**FastICA Algorithm**:
1. Whiten data
2. For each component:
   - Random initialization
   - Fixed-point iteration maximizing non-Gaussianity
   - Orthogonalize against previous components

**PCA vs ICA**:

| Aspect | PCA | ICA |
|--------|-----|-----|
| **Goal** | Maximize variance | Maximize independence |
| **Constraint** | Orthogonal | Independent |
| **Output** | Uncorrelated | Statistically independent |
| **Gaussianity** | Assumes Gaussian OK | Requires non-Gaussian |
| **Ordering** | By variance | Arbitrary |

**Applications**:
- Blind source separation (cocktail party problem)
- Signal processing (EEG, fMRI)
- Feature extraction
- Artifact removal

**Pros/Cons**:
- âœ… Finds independent components
- âœ… Works with non-Gaussian sources
- âœ… Useful for source separation
- âŒ Requires non-Gaussian sources
- âŒ Doesn't determine order or scale
- âŒ More computationally expensive than PCA
- âŒ Less stable than PCA

---

### t-SNE (t-Distributed Stochastic Neighbor Embedding)

**Purpose**: Nonlinear dimensionality reduction for visualization

**Algorithm**:

1. **Compute pairwise similarities in high-D**:
   ```
   p_{j|i} = exp(-||x_i-x_j||Â²/2Ïƒ_iÂ²) / Î£_{kâ‰ i} exp(-||x_i-x_k||Â²/2Ïƒ_iÂ²)
   p_{ij} = (p_{j|i} + p_{i|j})/(2m)
   ```

2. **Compute pairwise similarities in low-D**:
   ```
   q_{ij} = (1 + ||y_i-y_j||Â²)^(-1) / Î£_{kâ‰ l}(1 + ||y_k-y_l||Â²)^(-1)
   ```

3. **Minimize KL divergence**:
   ```
   KL(P||Q) = Î£_i Î£_j p_{ij}Â·log(p_{ij}/q_{ij})
   ```

**Hyperparameters**:
- **Perplexity**: Effective number of neighbors (5-50)
- **Learning rate**: Step size (100-1000)
- **Iterations**: Number of optimization steps

**Pros/Cons**:
- âœ… Excellent visualizations
- âœ… Captures local structure
- âœ… Handles non-linear manifolds
- âŒ Computationally expensive: `O(mÂ²)`
- âŒ Non-deterministic
- âŒ Cannot embed new points
- âŒ Global structure not preserved
- âŒ Sensitive to hyperparameters

**Best Practices**:
- Use for visualization only (2D/3D)
- Don't use for preprocessing
- Run multiple times with different perplexities
- Use PCA preprocessing for high dimensions

---

## Model Evaluation & Selection

### Classification Metrics

**Confusion Matrix**:

|              | Predicted Positive | Predicted Negative |
|--------------|-------------------|-------------------|
| **Actual Positive** | TP (True Positive)  | FN (False Negative) |
| **Actual Negative** | FP (False Positive) | TN (True Negative)  |

**Key Metrics**:

| Metric | Formula | When to Use |
|--------|---------|-------------|
| **Accuracy** | `(TP+TN)/(TP+TN+FP+FN)` | Balanced classes |
| **Precision** | `TP/(TP+FP)` | Cost of false positives high |
| **Recall/Sensitivity** | `TP/(TP+FN)` | Cost of false negatives high |
| **Specificity** | `TN/(TN+FP)` | True negative rate |
| **F1 Score** | `2TP/(2TP+FP+FN)` = `2Â·(PrecisionÂ·Recall)/(Precision+Recall)` | Imbalanced classes |
| **F-beta Score** | `(1+Î²Â²)Â·(PrecisionÂ·Recall)/(Î²Â²Â·Precision+Recall)` | Weight recall Î² times more |

**Interpretation**:
- **High Precision, Low Recall**: Few false positives, many false negatives (conservative)
- **Low Precision, High Recall**: Many false positives, few false negatives (liberal)

---

### ROC and PR Curves

**ROC (Receiver Operating Characteristic)**:
- X-axis: False Positive Rate (FPR) = `FP/(FP+TN)`
- Y-axis: True Positive Rate (TPR) = `TP/(TP+FN)` = Recall
- Shows trade-off across all thresholds

**AUC-ROC (Area Under Curve)**:
- Single metric summarizing ROC
- Perfect classifier: AUC = 1.0
- Random classifier: AUC = 0.5
- Interpretation: Probability model ranks random positive higher than random negative

**PR Curve (Precision-Recall)**:
- X-axis: Recall
- Y-axis: Precision
- Better for imbalanced datasets

**When to Use**:
- **ROC-AUC**: Balanced classes, care about both classes equally
- **PR-AUC**: Imbalanced classes, positive class more important

---

### Regression Metrics

**Sum of Squares**:
```
SStot = Î£(y_i - È³)Â²           # Total sum of squares
SSres = Î£(y_i - f(x_i))Â²      # Residual sum of squares
SSexp = Î£(f(x_i) - È³)Â²        # Explained sum of squares

SStot = SSexp + SSres
```

**RÂ² (Coefficient of Determination)**:
```
RÂ² = 1 - SSres/SStot = SSexp/SStot
```
- Range: (-âˆž, 1]
- 1 = perfect fit
- 0 = model no better than mean
- Negative = model worse than mean

**Adjusted RÂ²**:
```
RÌ„Â² = 1 - (1-RÂ²)Â·(m-1)/(m-p-1)
```
Penalizes additional variables (where `p` = number of predictors)

**Mean Squared Error (MSE)**:
```
MSE = (1/m)Â·Î£(y_i - Å·_i)Â²
```

**Root Mean Squared Error (RMSE)**:
```
RMSE = âˆšMSE
```
Same units as target variable

**Mean Absolute Error (MAE)**:
```
MAE = (1/m)Â·Î£|y_i - Å·_i|
```
More robust to outliers than MSE

**Mean Absolute Percentage Error (MAPE)**:
```
MAPE = (100/m)Â·Î£|y_i - Å·_i|/|y_i|
```
Scale-independent, interpretable as percentage

**Choosing Metrics**:
- **MSE/RMSE**: Penalize large errors more
- **MAE**: Robust to outliers
- **RÂ²**: Interpretable (proportion variance explained)
- **MAPE**: Scale-independent, but undefined when y=0

---

### Model Selection Criteria

**Information Criteria** (balance fit and complexity):

**AIC (Akaike Information Criterion)**:
```
AIC = 2k - 2ln(LÌ‚)
```
Where `k` = number of parameters, `LÌ‚` = max likelihood

**BIC (Bayesian Information Criterion)**:
```
BIC = kÂ·ln(m) - 2ln(LÌ‚)
```
Stronger penalty for complexity than AIC

**Mallow's Cp**:
```
Cp = SSres/ÏƒÌ‚Â² - m + 2p
```
Where `p` = number of predictors, `ÏƒÌ‚Â²` = estimate of error variance

**Rule**: Lower is better for all criteria

**When to Use**:
- **AIC**: Prediction focus
- **BIC**: Interpretation focus, prefers simpler models
- **Cp**: Linear regression specifically

---

### Cross-Validation

**k-Fold Cross-Validation**:

1. Partition data into `k` equal-sized folds
2. For each fold `i`:
   - Train on `k-1` folds
   - Validate on fold `i`
3. Average performance across all folds

**Typical**: k = 5 or k = 10

**Estimate**:
```
CV = (1/k)Â·Î£_{i=1}^k Error_i
```

**Leave-One-Out CV (LOOCV)**:
- Special case: `k = m`
- Unbiased but high variance
- Computationally expensive

**Leave-p-Out**:
- Test on `p` observations
- Train on `m - p`
- Very expensive: `C(m,p)` combinations

**Stratified k-Fold**:
- Maintains class distribution in each fold
- Essential for imbalanced data

**Time Series CV**:
- Respects temporal ordering
- Forward chaining: Expand training set, test on next period

**Nested CV**:
- Outer loop: Model evaluation
- Inner loop: Hyperparameter tuning
- Prevents overfitting to validation set

**Pros/Cons**:

| Method | Pros | Cons |
|--------|------|------|
| **Holdout** | Fast, simple | High variance, wastes data |
| **k-Fold** | Good bias-variance, efficient | Multiple trainings |
| **LOOCV** | Low bias, deterministic | High variance, expensive |
| **Stratified** | Handles imbalance | Only for classification |

---

### Data Partitioning

**Standard Split**:
- **Training**: 60-80% - Fit model
- **Validation**: 10-20% - Tune hyperparameters
- **Test**: 10-20% - Final evaluation

**Guidelines**:
- **Large data**: Can use smaller validation/test (e.g., 98/1/1)
- **Small data**: Use cross-validation instead of fixed split
- **Imbalanced**: Use stratified splitting
- **Time series**: Use temporal split (no shuffling)

**Never**:
- Touch test set during development
- Use test set for hyperparameter tuning
- Peek at test set distribution

---

## Learning Theory

### PAC Learning Framework

**Probably Approximately Correct (PAC)**:

A hypothesis class is PAC-learnable if:
- With probability â‰¥ `1-Î´` (probably)
- We can find hypothesis with error â‰¤ `Îµ` (approximately correct)
- Using polynomial samples and computation

**Assumptions**:
1. Training and test drawn from same distribution
2. Examples are independent and identically distributed (i.i.d.)

**Training Error**:
```
ÎµÌ‚(h) = (1/m)Â·Î£_{i=1}^m 1_{h(x^(i))â‰ y^(i)}
```

**Generalization Error**:
```
Îµ(h) = P_{(x,y)~D}[h(x) â‰  y]
```

**Goal**: Bound `Îµ(h)` using `ÎµÌ‚(h)`

---

### VC Dimension

**Shattering**: Hypothesis class `H` shatters set `S` if it can realize all `2^|S|` labelings

**VC Dimension**: Size of largest set that `H` can shatter
```
VC(H) = max{|S|: H shatters S}
```

**Examples**:

| Hypothesis Class | VC Dimension |
|------------------|--------------|
| Linear classifier in â„^n | n+1 |
| Intervals on â„ | 2 |
| Axis-aligned rectangles in â„Â² | 4 |
| Convex polygons in â„Â² | âˆž |
| Sin waves | âˆž |

**Interpretation**:
- Higher VC dimension = more expressive
- But also more data needed to learn

---

### Generalization Bounds

**Finite Hypothesis Class** (`|H| = k`):

With probability â‰¥ `1-Î´`:
```
Îµ(Ä¥) â‰¤ ÎµÌ‚(Ä¥) + âˆš[(1/2m)Â·log(2k/Î´)]
```

Or equivalently:
```
Îµ(Ä¥) â‰¤ min_{hâˆˆH} Îµ(h) + 2âˆš[(1/2m)Â·log(2k/Î´)]
```

**Infinite Hypothesis Class** (VC dimension `d`):

**Vapnik's Theorem**: With probability â‰¥ `1-Î´`:
```
Îµ(Ä¥) â‰¤ ÎµÌ‚(Ä¥) + O(âˆš[(d/m)Â·log(m/d) + (1/m)Â·log(1/Î´)])
```

**Sample Complexity** (number of examples needed):

For `Îµ(h) â‰¤ Îµ + Îµ*` with probability â‰¥ `1-Î´`:
```
m = O((d/ÎµÂ²)Â·log(1/Î´))
```

**Implications**:
1. More complex models (higher VC dimension) need more data
2. Generalization gap decreases with `âˆšm`
3. Cannot avoid dependence on VC dimension

---

### Bias-Variance Decomposition

**For regression** (`y = f(x) + Îµ`, where `E[Îµ] = 0`, `Var(Îµ) = ÏƒÂ²`):

Expected prediction error:
```
E[(y - Ä¥(x))Â²] = Bias(Ä¥(x))Â² + Var(Ä¥(x)) + ÏƒÂ²
```

Where:
- **Bias**: `E[Ä¥(x)] - f(x)` - systematic error
- **Variance**: `E[(Ä¥(x) - E[Ä¥(x)])Â²]` - sensitivity to training data
- **Irreducible error**: `ÏƒÂ²` - noise in data

**Trade-off**:
- Simple models: High bias, low variance
- Complex models: Low bias, high variance

**Optimal model complexity**: Minimize bias + variance

---

## Regularization & Optimization

### Regularization Techniques

**Purpose**: Prevent overfitting by constraining model complexity

**Ridge Regression (L2)**:
```
J(Î¸) = ||XÎ¸ - y||Â² + Î»||Î¸||â‚‚Â²
```

**Closed-form solution**:
```
Î¸Ì‚ = (X^T X + Î»I)^(-1) X^T y
```

**Effect**:
- Shrinks coefficients toward zero
- Never exactly zero
- Stable with correlated features
- Equivalent to Gaussian prior on `Î¸`

---

**LASSO (L1)**:
```
J(Î¸) = ||XÎ¸ - y||Â² + Î»||Î¸||â‚
```

**Effect**:
- Shrinks coefficients toward zero
- Can set coefficients exactly to zero (feature selection)
- Unstable with correlated features
- Equivalent to Laplace prior on `Î¸`

**No closed-form** - use coordinate descent or proximal methods

---

**Elastic Net**:
```
J(Î¸) = ||XÎ¸ - y||Â² + Î»[(1-Î±)||Î¸||â‚ + Î±||Î¸||â‚‚Â²]
```

Where `Î± âˆˆ [0,1]` balances L1/L2

**Effect**:
- Combines benefits of Ridge and LASSO
- Feature selection like LASSO
- Stability like Ridge
- Good for correlated features

**Special cases**:
- `Î± = 0`: Pure LASSO
- `Î± = 1`: Pure Ridge

---

### Choosing Regularization Parameter

**Î» Selection Methods**:

1. **Cross-validation**: Grid search over `Î»` values
2. **Regularization path**: Fit for sequence of `Î»`, plot coefficients
3. **Information criteria**: AIC/BIC with effective degrees of freedom

**Typical approach**:
```python
# Logarithmic grid
Î»_values = [10^(-3), 10^(-2), ..., 10^3]
# CV for each Î»
# Choose Î» with minimum CV error
```

**Effect of Î»**:
- `Î» = 0`: No regularization (may overfit)
- `Î» â†’ âˆž`: Maximum regularization (underfits, coefficients â†’ 0)

---

### Early Stopping

**For iterative methods** (gradient descent, neural networks):

1. Monitor validation error during training
2. Stop when validation error starts increasing
3. Return model from iteration with lowest validation error

**Equivalent to regularization**:
- Prevents model from fully fitting training data
- Simpler than explicit regularization
- Requires validation set

---

### Dropout (for Neural Networks)

**Training**: Randomly drop units with probability `p`

**Testing**: Use all units, scale by `(1-p)`

**Effect**:
- Prevents co-adaptation of features
- Ensemble effect
- Acts as regularization

---

### Data Augmentation

**Artificially expand training data**:

**Images**:
- Rotation, flipping, cropping
- Color jittering
- Gaussian noise

**Text**:
- Synonym replacement
- Back-translation
- Random insertion/deletion

**Time series**:
- Jittering, scaling
- Window slicing
- Magnitude warping

**Effect**: Regularization through more diverse training examples

---

## Bias-Variance Tradeoff

### Understanding the Tradeoff

**Learning Curves**: Plot training/validation error vs dataset size

**Three Regimes**:

#### 1. High Bias (Underfitting)
**Symptoms**:
- High training error
- Training â‰ˆ validation error
- Small gap between curves
- Both errors remain high as `m` increases

**Diagnosis**:
- Model too simple for data
- Not enough features
- Too much regularization

**Remedies**:
- âœ… Increase model complexity
- âœ… Add more features
- âœ… Reduce regularization (decrease `Î»`)
- âœ… Train longer
- âŒ More data won't help much

---

#### 2. High Variance (Overfitting)
**Symptoms**:
- Very low training error
- High validation error
- Large gap between curves
- Gap persists or increases with more data

**Diagnosis**:
- Model too complex
- Too many features
- Not enough data
- Too little regularization

**Remedies**:
- âœ… Get more training data
- âœ… Reduce model complexity
- âœ… Feature selection/reduction
- âœ… Increase regularization (increase `Î»`)
- âœ… Early stopping
- âœ… Ensemble methods
- âŒ Training longer makes it worse

---

#### 3. Good Fit (Sweet Spot)
**Symptoms**:
- Low training error
- Slightly higher validation error
- Small gap between curves
- Both errors decrease with more data

**Optimal**: Balance between bias and variance

---

### Debugging ML Models

**Performance Gap Analysis**:

```
Target Performance (Human/Bayes Error)
    â†“ (Unavoidable Bias)
Optimal Error (Best Possible)
    â†“ (Avoidable Bias)
Training Error
    â†“ (Variance)
Validation Error
    â†“ (Generalization)
Test Error
```

**Decision Tree**:

1. **Training error high?**
   - Yes â†’ High bias â†’ Increase complexity
   - No â†’ Go to 2

2. **Validation error high?**
   - Yes â†’ High variance â†’ More data / regularization
   - No â†’ Go to 3

3. **Test error high?**
   - Yes â†’ Validation set not representative
   - No â†’ Success!

---

### Error Analysis

**Systematic approach**:

1. **Analyze errors manually**:
   - Sample misclassified examples
   - Categorize error types
   - Identify patterns

2. **Quantify impact**:
   - Count errors by category
   - Estimate ceiling (if category fixed)

3. **Prioritize**:
   - Focus on most common error types
   - Consider ease of fixing

4. **Iterate**:
   - Make targeted improvements
   - Re-evaluate

**Example categories**:
- Mislabeled data
- Poor image quality
- Ambiguous examples
- Underrepresented classes

---

### Ablative Analysis

**Purpose**: Understand component contributions

**Method**:
1. Start with full system
2. Remove one component
3. Measure performance drop
4. Compare drops across components

**Insights**:
- Which features matter most
- Is preprocessing helping?
- Value of data cleaning

**Example**:
```
Full model:        95% accuracy
- Feature X:       93% (X contributes 2%)
- Feature Y:       94% (Y contributes 1%)
- Preprocessing:   90% (contributes 5%)
```

---

## Practical Tips

### Feature Engineering

**Scaling**:
- **Standardization**: `(x - Î¼)/Ïƒ` â†’ mean=0, std=1
- **Min-Max**: `(x - min)/(max - min)` â†’ range [0,1]
- **Robust**: Use median and IQR (outlier-resistant)

**When needed**:
- âœ… Gradient descent methods
- âœ… Distance-based methods (k-NN, k-means, SVM)
- âœ… Regularization (Ridge, LASSO)
- âŒ Tree-based methods

---

**Handling Categorical Variables**:

- **One-hot encoding**: Binary vector for each category
- **Label encoding**: Integer for each category (ordinal only)
- **Target encoding**: Mean target value per category
- **Frequency encoding**: Count/frequency per category

**Caution**: One-hot creates sparse, high-dimensional data

---

**Polynomial Features**:
```
xâ‚, xâ‚‚ â†’ xâ‚, xâ‚‚, xâ‚Â², xâ‚xâ‚‚, xâ‚‚Â²
```
Increases expressiveness but also complexity

---

**Interaction Features**:
Combine features: `xâ‚ Â· xâ‚‚`, `xâ‚/xâ‚‚`, etc.

---

### Handling Missing Data

**Strategies**:

1. **Remove**:
   - Drop rows (if few missing)
   - Drop columns (if mostly missing)

2. **Impute**:
   - Mean/median/mode
   - Forward/backward fill (time series)
   - Model-based (k-NN, regression)
   - Indicator variable (missingness flag)

3. **Predict**:
   - Treat as prediction problem
   - Use other features to predict missing values

**Choose based on**:
- Amount of missing data
- Missingness mechanism (random vs systematic)
- Model tolerance to missing values

---

### Handling Imbalanced Data

**Techniques**:

1. **Resampling**:
   - **Oversample minority**: Duplicate or synthesize (SMOTE)
   - **Undersample majority**: Remove examples
   - **Combined**: SMOTEENN, SMOTETomek

2. **Algorithm level**:
   - **Class weights**: Penalize majority errors less
   - **Threshold adjustment**: Move decision boundary
   - **Ensemble**: Balanced Random Forest

3. **Metric selection**:
   - Use F1, PR-AUC instead of accuracy
   - Focus on minority class metrics

4. **Anomaly detection**:
   - Treat as one-class problem
   - Model only majority class

---

### Hyperparameter Tuning

**Search Strategies**:

1. **Grid Search**:
   - Exhaustive over predefined grid
   - Guaranteed to find best in grid
   - Exponential in number of parameters

2. **Random Search**:
   - Sample randomly from distributions
   - More efficient for high dimensions
   - May miss optimal

3. **Bayesian Optimization**:
   - Model objective function
   - Intelligently choose next point
   - More efficient but complex

4. **Gradient-based**:
   - Differentiate through validation loss
   - Fast but requires differentiability

**Best Practices**:
- Start with random/grid search
- Use nested CV to avoid overfitting
- Search in log-space for learning rates
- Coarse-to-fine refinement

---

### Algorithm Selection Guide

**Decision Flowchart**:

```
Labeled data?
â”œâ”€ Yes (Supervised)
â”‚  â”œâ”€ Continuous output? â†’ Regression
â”‚  â”‚  â”œâ”€ Linear relationship? â†’ Linear Regression
â”‚  â”‚  â”œâ”€ Non-linear? â†’ Decision Tree, Random Forest, XGBoost
â”‚  â”‚  â””â”€ High-dimensional? â†’ Ridge, LASSO, Elastic Net
â”‚  â”‚
â”‚  â””â”€ Discrete output? â†’ Classification
â”‚     â”œâ”€ Linear boundary? â†’ Logistic Regression
â”‚     â”œâ”€ Complex boundary? â†’ SVM (RBF), Random Forest, XGBoost
â”‚     â”œâ”€ Probabilistic? â†’ Naive Bayes, Logistic Regression
â”‚     â”œâ”€ Interpretability? â†’ Decision Tree, Logistic Regression
â”‚     â””â”€ Text data? â†’ Naive Bayes, Logistic Regression
â”‚
â””â”€ No (Unsupervised)
   â”œâ”€ Grouping? â†’ Clustering
   â”‚  â”œâ”€ Spherical clusters? â†’ K-means
   â”‚  â”œâ”€ Arbitrary shapes? â†’ DBSCAN
   â”‚  â”œâ”€ Hierarchical? â†’ Agglomerative Clustering
   â”‚  â””â”€ Probabilistic? â†’ GMM
   â”‚
   â””â”€ Dimensionality reduction?
      â”œâ”€ Linear? â†’ PCA
      â”œâ”€ Independent sources? â†’ ICA
      â””â”€ Visualization? â†’ t-SNE
```

---

### Scikit-Learn Algorithm Selection Flowchart

The following flowchart is based on the [scikit-learn ML map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) and provides a practical decision tree for choosing algorithms based on sample size, problem type, and data characteristics.

```mermaid
flowchart TD
    Start([START]) --> SampleCheck{"> 50 samples?"}

    SampleCheck -->|No| GetData1[Get more data]
    SampleCheck -->|Yes| ProblemType{What are you<br/>trying to do?}

    ProblemType --> Category{Predicting<br/>a category?}
    ProblemType --> Quantity{Predicting<br/>a quantity?}
    ProblemType --> Exploring{Just looking/<br/>exploring?}

    %% Classification Path
    Category -->|Yes| HasLabels{Have labeled<br/>data?}
    HasLabels -->|No| GetData2[Get more data]
    HasLabels -->|Yes| ClassSize{"< 100K samples?"}

    ClassSize -->|Yes| TextData{Working with<br/>text data?}
    TextData -->|Yes| LinearSVC[Linear SVC]
    TextData -->|No| SGDClass1[SGD Classifier]

    ClassSize -->|No| SGDClass2[SGD Classifier]

    LinearSVC -.->|Not working| NB[Naive Bayes]
    NB -.->|Not working| KNN[K-Neighbors<br/>Classifier]
    KNN -.->|Not working| SVC[SVC]
    SVC -.->|Not working| EnsembleC[Ensemble<br/>Classifiers]

    %% Regression Path
    Quantity -->|Yes| RegSize{"< 100K samples?"}

    RegSize -->|Yes| FewFeatures1{Few features<br/>important?}
    FewFeatures1 -->|Yes| Lasso[Lasso/<br/>ElasticNet]
    FewFeatures1 -->|No| Ridge[Ridge Regression]

    Lasso -.->|Not working| SGDReg1[SGD Regressor]
    Ridge -.->|Not working| SVRLinear[SVR<br/>kernel='linear']
    SVRLinear -.->|Not working| EnsembleR1[Ensemble<br/>Regressors]

    RegSize -->|No| FewFeatures2{Few features<br/>important?}
    FewFeatures2 -->|Yes| SGDReg2[SGD Regressor]
    FewFeatures2 -->|No| SVFRBF[SVR<br/>kernel='rbf']

    SGDReg2 -.->|Not working| EnsembleR2[Ensemble<br/>Regressors]

    %% Clustering/Dimensionality Reduction Path
    Exploring -->|Yes| Structure{Predicting<br/>structure?}

    Structure -->|No| ToughLuck[Tough luck]
    Structure -->|Yes| ClusterOrDim{Have<br/>labeled data?}

    ClusterOrDim -->|No| ClusterSize{"< 10K samples?"}
    ClusterSize -->|Yes| KnownK{Know number<br/>of categories?}
    KnownK -->|Yes| KMeans[K-Means]
    KnownK -->|No| MeanShift[MeanShift]

    KMeans -.->|Not working| MiniBatch[MiniBatch<br/>K-Means]
    MiniBatch -.->|Not working| GMM[Gaussian Mixture<br/>Model]
    GMM -.->|Not working| Spectral[Spectral<br/>Clustering]

    ClusterSize -->|No| VBGMM[Variational Bayes<br/>GMM]

    ClusterOrDim -->|Yes| DimRed[Dimensionality<br/>Reduction]
    DimRed --> PCA[Randomized PCA]
    PCA --> DimSize{"< 10K samples?"}

    DimSize -->|Yes| IsoMap[Isomap/<br/>Spectral Embedding]
    DimSize -->|No| KernelApprox[Kernel<br/>Approximation]

    IsoMap -.->|Not working| LLE[Locally Linear<br/>Embedding]

    %% Styling
    classDef classifier fill:#e1f5ff,stroke:#0066cc,stroke-width:2px
    classDef regressor fill:#fff4e1,stroke:#cc6600,stroke-width:2px
    classDef cluster fill:#f0e1ff,stroke:#6600cc,stroke-width:2px
    classDef dimred fill:#e1ffe1,stroke:#00cc66,stroke-width:2px
    classDef decision fill:#fff,stroke:#333,stroke-width:2px
    classDef action fill:#ffe1e1,stroke:#cc0000,stroke-width:2px

    class LinearSVC,SGDClass1,SGDClass2,NB,KNN,SVC,EnsembleC classifier
    class Lasso,Ridge,SGDReg1,SGDReg2,SVRLinear,SVFRBF,EnsembleR1,EnsembleR2 regressor
    class KMeans,MiniBatch,GMM,Spectral,MeanShift,VBGMM cluster
    class PCA,IsoMap,LLE,KernelApprox,DimRed dimred
    class GetData1,GetData2,ToughLuck action
```

**Legend**:
- ðŸ”µ **Blue**: Classification algorithms
- ðŸŸ¡ **Orange**: Regression algorithms
- ðŸŸ£ **Purple**: Clustering algorithms
- ðŸŸ¢ **Green**: Dimensionality reduction
- ðŸ”´ **Red**: Action required (get more data)
- **Solid arrows**: Primary path
- **Dotted arrows**: Try if previous doesn't work

**Key Decision Points**:

| Decision | Threshold | Reasoning |
|----------|-----------|-----------|
| **Sample Size** | > 50 samples | Minimum for meaningful pattern detection |
| **Dataset Size** | < 100K samples | Determines if simpler/faster algorithms viable |
| **Cluster Size** | < 10K samples | Affects choice of clustering algorithm |
| **Dimension Reduction** | < 10K samples | Determines manifold learning vs approximation |

**Algorithm Fallback Chain**:

**Classification**:
```
Linear SVC â†’ Naive Bayes â†’ K-Neighbors â†’ SVC â†’ Ensemble Classifiers
```

**Regression**:
```
Lasso/ElasticNet â†’ SGD Regressor â†’ SVR â†’ Ensemble Regressors
```

**Clustering**:
```
K-Means â†’ MiniBatch K-Means â†’ GMM â†’ Spectral Clustering
```

**Dimensionality Reduction**:
```
Randomized PCA â†’ Isomap/Spectral Embedding â†’ LLE
```

---

### Computational Considerations

**Scalability**:

| Algorithm | Training Time | Prediction Time | Memory |
|-----------|--------------|----------------|--------|
| **Linear Regression** | O(nÂ²m) | O(n) | O(nm) |
| **Logistic Regression** | O(iterationsÂ·nm) | O(n) | O(nm) |
| **SVM** | O(mÂ² to mÂ³) | O(#SVÂ·n) | O(mÂ²) |
| **Decision Tree** | O(mn log m) | O(log m) | O(m) |
| **Random Forest** | O(TÂ·mn log m) | O(TÂ·log m) | O(TÂ·m) |
| **k-NN** | O(1) | O(mn) | O(nm) |
| **K-means** | O(iterationsÂ·kmn) | O(kn) | O((k+m)n) |
| **PCA** | O(min(mÂ²n, mnÂ²)) | O(kn) | O(mn) |

Where: `m` = samples, `n` = features, `k` = clusters/neighbors, `T` = trees

**Guidelines**:
- **Large m**: Use SGD, mini-batch, online algorithms
- **Large n**: Use feature selection, PCA, or regularization
- **Large m & n**: Deep learning, distributed computing
- **Real-time**: Pre-train complex model, use simple predictor

---

## Quick Reference Table

### Algorithm Comparison

| Algorithm | Type | Linearity | Probabilistic | Interpretable | Hyperparameters |
|-----------|------|-----------|---------------|---------------|-----------------|
| **Linear Regression** | Regression | Linear | Yes | High | None (or Î») |
| **Logistic Regression** | Classification | Linear | Yes | High | Î» |
| **SVM** | Classification | Non-linear (kernel) | No | Low | C, kernel params |
| **Decision Tree** | Both | Non-linear | No | High | max_depth, min_samples |
| **Random Forest** | Both | Non-linear | Yes (voting) | Medium | n_trees, max_depth |
| **XGBoost** | Both | Non-linear | Yes (voting) | Low | Î·, max_depth, n_estimators |
| **k-NN** | Both | Non-linear | Yes | Medium | k, distance metric |
| **Naive Bayes** | Classification | Non-linear | Yes | High | Smoothing |
| **K-means** | Clustering | Linear | No | High | k |
| **GMM** | Clustering | Non-linear | Yes | Medium | k, covariance type |
| **PCA** | Dim. Reduction | Linear | No | Medium | n_components |

---

## Mathematical Foundations Summary

### Matrix Calculus

**Gradient of scalar w.r.t. vector**:
```
âˆ‡_x f(x) = [âˆ‚f/âˆ‚xâ‚, ..., âˆ‚f/âˆ‚x_n]^T
```

**Common derivatives**:
- `âˆ‡_x (a^T x) = a`
- `âˆ‡_x (x^T A x) = (A + A^T)x`
- `âˆ‡_x (||x||Â²) = 2x`

**Chain rule**:
```
âˆ‚f/âˆ‚x = (âˆ‚f/âˆ‚y)Â·(âˆ‚y/âˆ‚x)
```

---

### Probability Essentials

**Bayes' Theorem**:
```
P(A|B) = P(B|A)Â·P(A) / P(B)
```

**Law of Total Probability**:
```
P(B) = Î£_i P(B|A_i)Â·P(A_i)
```

**Independence**:
```
P(A,B) = P(A)Â·P(B)
```

**Conditional Independence**:
```
P(A,B|C) = P(A|C)Â·P(B|C)
```

**Expectation**:
```
E[X] = Î£ xÂ·P(X=x)    (discrete)
E[X] = âˆ« xÂ·p(x)dx    (continuous)
```

**Variance**:
```
Var(X) = E[(X - E[X])Â²] = E[XÂ²] - (E[X])Â²
```

**Covariance**:
```
Cov(X,Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]
```

---

### Common Distributions

| Distribution | PMF/PDF | Mean | Variance |
|--------------|---------|------|----------|
| **Bernoulli(Ï†)** | `Ï†^x(1-Ï†)^(1-x)` | `Ï†` | `Ï†(1-Ï†)` |
| **Binomial(n,Ï†)** | `C(n,k)Ï†^k(1-Ï†)^(n-k)` | `nÏ†` | `nÏ†(1-Ï†)` |
| **Gaussian(Î¼,ÏƒÂ²)** | `(1/âˆš(2Ï€ÏƒÂ²))exp(-(x-Î¼)Â²/2ÏƒÂ²)` | `Î¼` | `ÏƒÂ²` |
| **Multinomial(n,Ï†)** | `(n!)/(Î  x_i!)Â·Î  Ï†_i^x_i` | `nÏ†_i` | `nÏ†_i(1-Ï†_i)` |
| **Poisson(Î»)** | `(Î»^k/k!)exp(-Î»)` | `Î»` | `Î»` |

---

## Checklist for ML Projects

### 1. Problem Definition
- [ ] Define success metrics
- [ ] Establish baseline performance
- [ ] Determine if ML is appropriate

### 2. Data Collection & Exploration
- [ ] Gather sufficient data
- [ ] Visualize data distribution
- [ ] Check for missing values
- [ ] Identify outliers
- [ ] Analyze correlations
- [ ] Check class balance (classification)

### 3. Data Preprocessing
- [ ] Handle missing data
- [ ] Remove or cap outliers
- [ ] Encode categorical variables
- [ ] Scale/normalize features
- [ ] Split train/val/test sets
- [ ] Check for data leakage

### 4. Feature Engineering
- [ ] Create domain-specific features
- [ ] Generate polynomial/interaction features
- [ ] Feature selection
- [ ] Dimensionality reduction (if needed)

### 5. Model Selection
- [ ] Try simple baseline (mean, mode, etc.)
- [ ] Experiment with multiple algorithms
- [ ] Consider interpretability needs
- [ ] Evaluate computational constraints

### 6. Training & Tuning
- [ ] Choose appropriate loss function
- [ ] Set aside validation set
- [ ] Tune hyperparameters (CV)
- [ ] Monitor learning curves
- [ ] Apply regularization if needed
- [ ] Use early stopping

### 7. Evaluation
- [ ] Test on hold-out set
- [ ] Analyze confusion matrix (classification)
- [ ] Plot ROC/PR curves (classification)
- [ ] Compute relevant metrics
- [ ] Perform error analysis
- [ ] Check for bias in predictions

### 8. Iteration
- [ ] Identify failure modes
- [ ] Gather more data (if high variance)
- [ ] Add features (if high bias)
- [ ] Try different algorithms
- [ ] Ensemble models

### 9. Deployment Considerations
- [ ] Document model assumptions
- [ ] Create inference pipeline
- [ ] Monitor model performance
- [ ] Plan for model updates
- [ ] Consider fairness & ethics

---

## Common Pitfalls to Avoid

1. **Data Leakage**: Test data information in training
2. **Ignoring Class Imbalance**: Using accuracy on imbalanced data
3. **Overfitting to Validation Set**: Tuning on test set
4. **Not Scaling Features**: For distance-based or gradient methods
5. **Forgetting Regularization**: Especially with high-dimensional data
6. **Ignoring Outliers**: Can severely affect some models
7. **Using Wrong Metric**: Accuracy for imbalanced, MSE with outliers
8. **Not Checking Assumptions**: Linear models assume linearity
9. **Correlation â‰  Causation**: Be careful with interpretation
10. **Extrapolation**: Models may fail outside training distribution

---

## Resources & Further Reading

**Foundational Papers**:
- Vapnik & Chervonenkis: Statistical Learning Theory
- Breiman: Random Forests (2001)
- Friedman: Greedy Function Approximation (Gradient Boosting, 1999)
- van der Maaten & Hinton: t-SNE (2008)

**Textbooks**:
- *Pattern Recognition and Machine Learning* - Bishop
- *The Elements of Statistical Learning* - Hastie, Tibshirani, Friedman
- *Machine Learning: A Probabilistic Perspective* - Murphy
- *Understanding Machine Learning* - Shalev-Shwartz, Ben-David

**Online Courses**:
- Stanford CS229: Machine Learning (Andrew Ng)
- Coursera: Machine Learning Specialization
- Fast.ai: Practical Deep Learning

---

*This cheatsheet compiled from Stanford CS-229 materials. For the latest updates and detailed derivations, refer to course materials at stanford.edu/~shervine/teaching/cs-229/*
