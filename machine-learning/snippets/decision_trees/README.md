# Decision Trees - CART Implementation

Educational implementation of Classification and Regression Trees from scratch.

## üìÅ Files

```
tree_node.py                   # Node data structure
tree_metrics.py                # Splitting criteria (Gini, Entropy, MSE, MAE)
base_tree.py                   # Core CART algorithm
decision_tree_classifier.py    # Classification tree
decision_tree_regressor.py     # Regression tree
examples_decision_tree.py      # Usage examples
```

## üèóÔ∏è Architecture

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ      TreeMetrics        ‚îÇ
                    ‚îÇ  (Gini, Entropy, MSE)   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                            uses‚îÇ
                                ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   BaseDecisionTree      ‚îÇ
                    ‚îÇ   (CART Algorithm)      ‚îÇ
                    ‚îÇ  ‚Ä¢ _grow_tree()         ‚îÇ
                    ‚îÇ  ‚Ä¢ _find_best_split()   ‚îÇ
                    ‚îÇ  ‚Ä¢ predict()            ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ                       ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ  DecisionTree     ‚îÇ   ‚îÇ  DecisionTree   ‚îÇ
          ‚îÇ   Classifier      ‚îÇ   ‚îÇ   Regressor     ‚îÇ
          ‚îÇ                   ‚îÇ   ‚îÇ                 ‚îÇ
          ‚îÇ ‚Ä¢ Gini/Entropy    ‚îÇ   ‚îÇ ‚Ä¢ MSE/MAE       ‚îÇ
          ‚îÇ ‚Ä¢ Predict mode    ‚îÇ   ‚îÇ ‚Ä¢ Predict mean  ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ                      ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                        builds ‚îÇ
                               ‚îÇ
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚îÇ    Node   ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Quick Start

### Classification

```python
from snippets.decision_trees import DecisionTreeClassifier
import numpy as np

X_train = np.array([[2, 3], [1, 1], [6, 6], [7, 5]])
y_train = np.array([0, 0, 1, 1])

clf = DecisionTreeClassifier(criterion='gini', max_depth=5)
clf.fit(X_train, y_train)

predictions = clf.predict(np.array([[1, 2], [6, 5]]))
print(predictions)  # [0, 1]
```

### Regression

```python
from snippets.decision_trees import DecisionTreeRegressor
import numpy as np

X_train = np.array([[1], [2], [3], [8], [9], [10]])
y_train = np.array([5.0, 5.1, 4.9, 10.2, 9.8, 10.1])

reg = DecisionTreeRegressor(criterion='mse', max_depth=5)
reg.fit(X_train, y_train)

predictions = reg.predict(np.array([[2], [9]]))
print(predictions)  # [5.0, 10.03]
```

## üîç How It Works

**CART Algorithm:**
1. Start with all data at root
2. Try all possible splits (each feature √ó each threshold)
3. Pick split with highest information gain (classification) or variance reduction (regression)
4. Recursively split left and right children
5. Stop when max_depth reached, too few samples, or no good split
6. Leaf nodes predict: most common class (classification) or mean/median (regression)

**Key Idea:** TreeMetrics calculates split quality at each step.

## üå≥ Understanding Recursive Tree Building

The tree is built using **recursion** - the `_grow_tree()` method calls itself to build left and right subtrees. Here's how it works step by step:

### Recursive Flow

```
_grow_tree(all_data, depth=0)
‚îÇ
‚îú‚îÄ Check stopping criteria (max depth, min samples, pure node)
‚îú‚îÄ If should stop ‚Üí create LEAF node, return ‚úÖ
‚îÇ
‚îú‚îÄ Find best split across all features and thresholds
‚îú‚îÄ If no good split ‚Üí create LEAF node, return ‚úÖ
‚îÇ
‚îî‚îÄ Otherwise:
   ‚îú‚îÄ Split data into left (‚â§ threshold) and right (> threshold)
   ‚îú‚îÄ left_child = _grow_tree(left_data, depth+1)   ‚Üê RECURSE
   ‚îú‚îÄ right_child = _grow_tree(right_data, depth+1) ‚Üê RECURSE
   ‚îî‚îÄ Return INTERNAL node with children ‚úÖ
```

### Visual Example: Building a Classification Tree

Let's walk through building a tree for this toy dataset:

```python
# Dataset: predict if person will buy (0=No, 1=Yes)
# Features: [Age, Income]
X = [[25, 30k], [30, 40k], [35, 50k], [40, 60k], [45, 70k]]
y = [0,         0,         1,         1,         1        ]
```

**Step-by-step tree growth:**

```
CALL 1: _grow_tree(all 5 samples, depth=0)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Root: 5 samples, [0,0,1,1,1]                ‚îÇ
‚îÇ Best split: Age ‚â§ 32 (separates well)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚ñº           ‚ñº           ‚ñº
   Split into:  RECURSE      RECURSE
                 LEFT         RIGHT
   Left:  [25,30] ‚Üí [0,0]
   Right: [35,40,45] ‚Üí [1,1,1]


CALL 2: _grow_tree(left: 2 samples [0,0], depth=1)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Left child: 2 samples, all class 0          ‚îÇ
‚îÇ PURE NODE! ‚Üí Create LEAF predicting 0       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚îî‚îÄ‚Üí RETURN Node(value=0) ‚úÖ


CALL 3: _grow_tree(right: 3 samples [1,1,1], depth=1)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Right child: 3 samples, all class 1         ‚îÇ
‚îÇ PURE NODE! ‚Üí Create LEAF predicting 1       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚îî‚îÄ‚Üí RETURN Node(value=1) ‚úÖ


BACK TO CALL 1:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Both children returned                      ‚îÇ
‚îÇ Create INTERNAL node:                       ‚îÇ
‚îÇ   - Split: Age ‚â§ 32                         ‚îÇ
‚îÇ   - Left: Node(value=0)                     ‚îÇ
‚îÇ   - Right: Node(value=1)                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚îî‚îÄ‚Üí RETURN Node(split, left, right) ‚úÖ
```

**Final Tree:**

```
            [Root: Age ‚â§ 32?]
                 /    \
                /      \
         [Yes] /        \ [No]
              /          \
          Predict 0    Predict 1
         (2 samples)  (3 samples)
```

### Code Trace Through base_tree.py

Here's what happens in the code:

```python
# Initial call
self.root = self._grow_tree(X, y, depth=0)

def _grow_tree(X, y, depth):
    # 1. Check stopping criteria
    if depth >= max_depth:           # ‚Üê Stop condition
        return Node(value=leaf_value) # ‚Üê BASE CASE (recursion stops)

    if n_samples < min_samples_split:
        return Node(value=leaf_value) # ‚Üê BASE CASE

    if all_same_class(y):            # Pure node
        return Node(value=leaf_value) # ‚Üê BASE CASE

    # 2. Find best split
    best_feature, best_threshold, gain = _find_best_split(X, y)

    if gain <= 0:                     # No good split
        return Node(value=leaf_value) # ‚Üê BASE CASE

    # 3. Split data
    left_mask = X[:, best_feature] <= best_threshold
    X_left, y_left = X[left_mask], y[left_mask]
    X_right, y_right = X[~left_mask], y[~left_mask]

    # 4. RECURSIVE CALLS - Build subtrees
    left_child = self._grow_tree(X_left, y_left, depth+1)   # ‚Üê RECURSION
    right_child = self._grow_tree(X_right, y_right, depth+1) # ‚Üê RECURSION

    # 5. Return internal node
    return Node(
        feature_idx=best_feature,
        threshold=best_threshold,
        left=left_child,      # ‚Üê Children already built!
        right=right_child     # ‚Üê Children already built!
    )
```

### Key Insights

1. **Base Cases Stop Recursion**: Leaf nodes are created when stopping criteria are met (max depth, pure node, etc.)

2. **Depth Increases Each Level**: `depth+1` ensures the tree eventually hits `max_depth` and stops

3. **Data Gets Smaller**: Each recursive call works with a subset of the parent's data (split by threshold)

4. **Bottom-Up Construction**: Recursion builds leaves first (deepest calls), then works back up to build parent nodes

5. **Each Call Returns a Node**: Either a leaf (base case) or an internal node with children (recursive case)

### Deeper Tree Example

For a tree with `max_depth=2`, the recursion goes deeper:

```
Depth 0: _grow_tree(100 samples)
         ‚îú‚îÄ Depth 1: _grow_tree(60 samples, left)
         ‚îÇ           ‚îú‚îÄ Depth 2: _grow_tree(40 samples) ‚Üí LEAF ‚úÖ
         ‚îÇ           ‚îî‚îÄ Depth 2: _grow_tree(20 samples) ‚Üí LEAF ‚úÖ
         ‚îÇ
         ‚îî‚îÄ Depth 1: _grow_tree(40 samples, right)
                     ‚îú‚îÄ Depth 2: _grow_tree(25 samples) ‚Üí LEAF ‚úÖ
                     ‚îî‚îÄ Depth 2: _grow_tree(15 samples) ‚Üí LEAF ‚úÖ
```

The recursion naturally creates the tree structure by:
- **Going deep** (recursive calls)
- **Hitting base cases** (creating leaves)
- **Returning back up** (building internal nodes)

## ‚öôÔ∏è Hyperparameters

- **criterion**: `'gini'` or `'entropy'` (classification), `'mse'` or `'mae'` (regression)
- **max_depth**: Tree depth limit (3-10 typically good, `None` = unlimited ‚ö†Ô∏è overfits)
- **min_samples_split**: Min samples to split a node (default: 2)
- **min_samples_leaf**: Min samples in leaf (default: 1)

## üÜö Comparison with scikit-learn

Our educational implementation vs `sklearn.tree.DecisionTree*`:

| Feature | Our Implementation | scikit-learn |
|---------|-------------------|--------------|
| **Purpose** | Educational, learn CART | Production-ready |
| **Code clarity** | ‚úÖ Clean, well-commented | Optimized Cython |
| **Speed** | Slow (pure Python) | ‚ö° Fast (C backend) |
| **Pruning** | ‚ùå None | ‚úÖ Cost-complexity pruning |
| **Missing values** | ‚ùå Not supported | ‚úÖ Supported |
| **Categorical features** | ‚ùå Must encode manually | ‚úÖ Built-in support |
| **Parallelization** | ‚ùå Single-threaded | ‚úÖ Multi-threaded |
| **API compatibility** | Similar `fit`/`predict` | Standard sklearn API |
| **Good for** | Understanding how it works | Real applications |

**scikit-learn Documentation:**
- [DecisionTreeClassifier API Reference](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)
- [DecisionTreeRegressor API Reference](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)
- [Decision Trees User Guide](https://scikit-learn.org/stable/modules/tree.html)

## üß™ Run Examples

```bash
# From machine-learning/snippets/ directory
cd machine-learning/snippets
python -m decision_trees.examples_decision_tree
```

Includes: classification, regression, overfitting demo, outlier robustness.

## üìñ Learn More

- **[../../notes/decision-trees.md](../../notes/decision-trees.md)** - Complete theory guide
- **[../../notes/tree-metrics.md](../../notes/tree-metrics.md)** - Metric explanations

Topics: CART algorithm, splitting criteria, hyperparameter tuning, common pitfalls.

---

**Next:** Build Tree Ensemble Models on top of this!
